bvec=c(1,muP[i],rep(0,3)),meq=2)
sdP[i] = sqrt(result$value)
weight[i,] = result$solution
}
par(mfrow = c(1,1))
plot(sdP,muP,type="l",xlim=c(.01,.03),ylim=c(0,.0011),lwd=3,col="blue", main = "Efficient Frontier") # efficient frontier
#lines(sdP,muP,type="l",xlim=c(0,.04),ylim=c(0,.001),lwd=3,col="blue") # efficient frontier
text(sqrt(sigma[1,1]),mu[1],"CAT",cex=1.1)
text(sqrt(sigma[2,2]),mu[2],"IBM",cex=1.1)
text(sqrt(sigma[3,3]),mu[3],"MSFT",cex=1.1)
shiny::runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
shiny::runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
data1 <- read.table("FamaFrench_mon_69_98.txt")
View(data1)
data1 <- read.table("FamaFrench_mon_69_98.txt", header = TRUE)
View(data1)
data1 <- read.table("FamaFrench_mon_69_98.txt", header = TRUE)
View(data1)
View(data1)
install.packages("Ecdat")
library("Ecdat")
library("Ecdat")
data2 <- data(CRSPmon)
library("Ecdat")
data2 <- data(CRSPmon)
data2 <- data(CRSPmon)
data(CRSPmon)
data(CRSPmon)
data(CRSPmon)
data2 <- data("CRSPmon")
data("CRSPmon")
data2
library("Ecdat")
data("CRSPmon")
data2 <- CRSPmon
data2
pca = prcomp(data2,scale = TRUE)
pca <- prcomp(data2,scale = TRUE)
pca
summary(pca)
summary(pca1)
pca1 <- prcomp(data2,scale = TRUE)
pca1
summary(pca1)
?prcomp
pca1
summary(pca1)
pca1
pca1 <- prcomp(data2[,1:3],scale = TRUE)
pca1
summary(pca1)
pca1
summary(pca1)
plot(pca1)
pca1
summary(pca1)
pca1
head(data2)
data2
data2[3]
head(data2)
data2 <- data.frame(data2)
View(data2)
pc11 <- c(0.6284832, 0.5937757, 0.5024334) * data2[1, 1:3]
View(pc11)
data2 <- data.frame(data2)
pc11 <- c(0.6284832, 0.5937757, 0.5024334) * data2[1, 1:3]
pc12 <- c(-0.2093695, -0.4929652, 0.8444819) * data2[1, 1:3]
pc13 <- c(-0.7491150, 0.6359369, 0.1855019) * data2[1, 1:3]
pc21 <- c(0.6284832, 0.5937757, 0.5024334) * data2[2, 1:3]
pc22 <- c(-0.2093695, -0.4929652, 0.8444819) * data2[2, 1:3]
pc23 <- c(-0.7491150, 0.6359369, 0.1855019) * data2[2, 1:3]
pca1
pca$x
pca$x[1:2, 1:3]
data1 <- read.table("FamaFrench_mon_69_98.txt", header = TRUE)
library("Ecdat")
data2 <- CRSPmon
pca1 <- prcomp(data2[,1:3],scale = TRUE)
pca1
summary(pca1)
pca1$x[1:2, 1:3]
pca1$x[1:2, 1:3]
pca1$x[1:2, ]
View(data1)
pca1$x[1:2, ]
head(pca1$x)
ge <- 100*CRSPmon[1:180, 1] - RF
ge <- 100*CRSPmon[1:180, 1] - data1$RF
ibm <- 100*CRSPmon[1:180, 2] - data1$RF
mobil <- 100*CRSPmon[1:180, 3] - data1$RF
stocks <- cbind(ge,ibm,mobil)
fit <- lm(cbind(ge,ibm,mobil)~Mkt.RF+SMB+HML)
ge <- 100*CRSPmon[1:180, 1] - data1$RF
ibm <- 100*CRSPmon[1:180, 2] - data1$RF
mobil <- 100*CRSPmon[1:180, 3] - data1$RF
stocks <- cbind(ge,ibm,mobil)
fit <- lm(cbind(ge,ibm,mobil)~Mkt.RF+SMB+HML, data = data1)
fit
pca1$x[1:2, ]
ge <- 100*CRSPmon[1:180, 1] - data1$RF
ibm <- 100*CRSPmon[1:180, 2] - data1$RF
mobil <- 100*CRSPmon[1:180, 3] - data1$RF
stocks <- cbind(ge,ibm,mobil)
fit <- lm(cbind(ge,ibm,mobil)~Mkt.RF[1:180] + SMB[1:180] + HML[1:180], data = data1)
ge <- 100*CRSPmon[1:180, 1] - data1$RF[1:180]
ibm <- 100*CRSPmon[1:180, 2] - data1$RF[1:180]
mobil <- 100*CRSPmon[1:180, 3] - data1$RF[1:180]
stocks <- cbind(ge,ibm,mobil)
fit <- lm(cbind(ge,ibm,mobil) ~ Mkt.RF[1:180] + SMB[1:180] + HML[1:180], data = data1)
fit
ge1 <- 100*CRSPmon[181:360, 1] - data1$RF[181:360]
ibm1 <- 100*CRSPmon[181:360, 2] - data1$RF[181:360]
mobil1 <- 100*CRSPmon[181:360, 3] - data1$RF[181:360]
stocks1 <- cbind(ge1,ibm1,mobil1)
fit1 <- lm(cbind(ge1,ibm1,mobil1) ~ Mkt.RF[181:360] + SMB[181:360] + HML[181:360], data = data1)
fit1
ge <- 100*CRSPmon[1:180, 1] - data1$RF[1:180]
ibm <- 100*CRSPmon[1:180, 2] - data1$RF[1:180]
mobil <- 100*CRSPmon[1:180, 3] - data1$RF[1:180]
stocks <- cbind(ge,ibm,mobil)
fit <- lm(cbind(ge,ibm,mobil) ~ Mkt.RF[1:180] + SMB[1:180] + HML[1:180], data = data1)
fit
shiny::runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
runApp('C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project2-Group8/app')
install.packages("mvtnorm")
library(mvtnorm)
library(mvtnorm)
FT = 0.0346
C = pmvnorm(lower=c(-Inf,-Inf),upper=c(qnorm(FT),qnorm(FT)),
sigma=matrix(c(1,0.5,0.5,1),2,2))
val1 <- 1000000*exp(-0.04*1)*(FT+FT-C)
val1
1000000*exp(-0.04*1)*(FT+FT-C)
val1 <- 1000000*exp(-0.04*1)*(FT+FT-C)
val1
val1[1]
val2 <- 1000000*exp(-0.04*1)*C
val2
rbinom
?rbinom
binorm
rbinom(1,5,0.5)
pnorm(0.018,0,1)
pnorm(0.015,0,1)
pnorm(0.015,-0.001,0.015)
1-pnorm(0.015,-0.001,0.015)
pnorm(0.018,0.002,0.016)
1-pnorm(0.018,0.002,0.016)
qnorm(0.9,0,1)
1000000*(1.28*0.016-0.002)
qnorm(0.95,0,1)
1000*(1.64*0.2-0.1)
((18230/1000000)+0.002)/0.016
qt(0.9,2)
?qt
1000*(1.89*0.016-0.002)
qt(0.9,5)
qt(0.9,2)
1000*(1.48*0.016-0.002)
1000000*(1.64*(0.15+0.5*0.5*0.5*0.15)-0.05)
1000000*(1.64*0.5*0.5*0.5*0.15-0.05)
1000000*(1.64*0.5*0.15-0.05)
pnorm(1.64,0,1)
dnorm(1.64,0,1)
install.packages("survival")
install.packages("MASS")
install.packages("muhaz")
?dnorm
z1 = c(0,42,51,53,60,64)
ev1 = c(1,1*7/8, 1*7/8*4/7,1*7/8*4/7*1,1*7/8*4/7*1*2/3,0)
plot(z1,ev1,type = "s")
points(53,1*7/8*4/7,pch = 3)
points(60,1*7/8*4/7*1*2/3,pch=3)
z2 = c(42,51,51,51,53,60,60,64)
ev2 = c(1,1,1,1,0,1,0,1)
plot(survfit(Surv(z2,ev2)~1),conf.int= FALSE, mark.time = TRUE)
z2 = c(42,51,51,51,53,60,60,64)
ev2 = c(1,1,1,1,0,1,0,1)
plot(survfit(Surv(z2,ev2)~1),conf.int= FALSE, mark.time = TRUE)
library(survival)
library(MASS)
library(muhaz)
library(survival)
library(MASS)
library(muhaz)
z1 = c(0,42,51,53,60,64)
ev1 = c(1,1*7/8, 1*7/8*4/7,1*7/8*4/7*1,1*7/8*4/7*1*2/3,0)
plot(z1,ev1,type = "s")
points(53,1*7/8*4/7,pch = 3)
points(60,1*7/8*4/7*1*2/3,pch=3)
library(survival)
library(MASS)
library(muhaz)
z1 = c(0,42,51,53,60,64)
ev1 = c(1,1*7/8, 1*7/8*4/7,1*7/8*4/7*1,1*7/8*4/7*1*2/3,0)
plot(z1,ev1,type = "s")
points(53,1*7/8*4/7,pch = 3)
points(60,1*7/8*4/7*1*2/3,pch=3)
z2 = c(42,51,51,51,53,60,60,64)
ev2 = c(1,1,1,1,0,1,0,1)
plot(survfit(Surv(z2,ev2)~1),conf.int= FALSE, mark.time = TRUE)
##if TRUE, then curves are marked at each censoring time which is not also a death time.
train_5 <- read.csv("train.5.txt", header = FALSE)
train_6 <- read.csv("train.6.txt", header = FALSE)
labels <- rep(c(-1,1), c(nrow(train_5), nrow(train_6)))
labels <- as.factor(labels)
library(e1071)
train_complete <- rbind(train_5, train_6)
train_complete <- cbind(labels, train_complete)
View(train_complete)
?sample
rows <- nrow(train_complete)
test_samples <- sample(1:rows, 0.2*rows)
set.seed(2018)
test_samples <- sample(1:rows, 0.2*rows)
test_samples <- sample(1:rows, 0.2*rows)
set.seed(2018)
test_samples <- sample(1:rows, 0.2*rows)
library(e1071)
train_5 <- read.csv("train.5.txt", header = FALSE)
train_6 <- read.csv("train.6.txt", header = FALSE)
labels <- rep(c(-1,1), c(nrow(train_5), nrow(train_6)))
labels <- as.factor(labels)
data_complete <- rbind(train_5, train_6)
data_complete <- cbind(labels, train_complete)
rows <- nrow(data_complete)
set.seed(2018)
test_samples <- sample(1:rows, 0.2*rows)
train <- data_complete[-test_samples, ]
test <- data_complete[test_samples, ]
?tune.svm
svm.linear <- tune.svm(labels ~ ., data = data_complete)
svm.linear <- tune.svm(labels ~ ., data = data_complete, cost = c(10^(-4:4)), kernel = "linear")
svm.linear <- tune.svm(labels ~ ., data = data_complete, cost = c(10^(-4:4)), kernel = "linear")
svm.linear <- tune.svm(x = data_complete[, -1], y = data_complete[, 1], cost = c(10^(-4:4)), kernel = "linear")
svm.linear <- tune.svm(x = train, y = test, cost = c(10^(-4:4)), kernel = "linear")
View(train)
svm.linear <- tune.svm(labels ~ ., data = train, cost = c(10^(-4:4)), kernel = "linear")
seq(0.001, 0.08, 0.002)
library(e1071)
train_5 <- read.csv("train.5.txt", header = FALSE)
train_6 <- read.csv("train.6.txt", header = FALSE)
labels <- rep(c(-1,1), c(nrow(train_5), nrow(train_6)))
labels <- as.factor(labels)
data_complete <- rbind(train_5, train_6)
data_complete <- cbind(labels, train_complete)
library(e1071)
train_5 <- read.csv("train.5.txt", header = FALSE)
train_6 <- read.csv("train.6.txt", header = FALSE)
labels <- rep(c(-1,1), c(nrow(train_5), nrow(train_6)))
labels <- as.factor(labels)
data_complete <- rbind(train_5, train_6)
data_complete <- cbind(labels, data_complete)
rows <- nrow(data_complete)
set.seed(2018)
test_samples <- sample(1:rows, 0.2*rows)
train <- data_complete[-test_samples, ]
test <- data_complete[test_samples, ]
svm.linear <- tune.svm(labels ~ ., data = train, cost = c(10^(-4:4)), kernel = "linear")
svm.linear
svm.linear
svm.linear <- tune.svm(labels ~ ., data = train, cost = seq(0.001, 0.08, 0.002), kernel = "linear")
svm.linear
svm.linear <- tune.svm(labels ~ ., data = train, cost = seq(0.001, 0.06, 0.002), kernel = "linear")
svm.linear
seq(1, 10, 2)
seq(0.004, 0.04, 0.004)
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.01, 0.6, 0.02), gamma = seq(0.001, 0.06, 0.002))
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.01, 0.6, 0.02), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.01, 0.6, 0.02), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.002, 0.08, 0.002), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
plot(svm.linear)
digit5 = read.table(paste(base_loc, "train.5.txt", sep=""), header=F, sep=",")
digit6 = read.table(paste(base_loc, "train.6.txt", sep=""), header=F, sep=",")
n1 = dim(digit5)[1]
n2 = dim(digit6)[1]
p =  dim(digit5)[2]
## set training and testing
test_prop = 0.2
index1 = sample(n1, round(test_prop * n1), replace=F)  # index for testing
index2 = sample(n2, round(test_prop * n2), replace=F)
xtest  = rbind(digit5[index1, ], digit6[index2, ])
xtrain = rbind(digit5[-index1, ], digit6[-index2, ])
ytest  = factor(c(rep("5", length(index1)), rep("6", length(index2))))
ytrain = factor(c(rep("5", n1 - length(index1)), rep("6", n2 - length(index2))))
## Q2, Q3: perform linear / RBF - SVM with k = 10 fold CV
# default for tune.svm() is 10-fold CV
tune_svm1 = tune.svm(x = xtrain, y = ytrain,
cost = seq(0.001, 0.08, 0.002), kernel="linear")
#pdf(width = 7, height = 7, file = paste(base_loc, "3-1.pdf", sep=""))
plot(tune_svm1, main = "tuning on Cost(C) of linear-SVM")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.002, 0.08, 0.002), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.02, 0.08, 0.002), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(1, 10, 2), gamma = seq(0.004, 0.04, 0.004),
kernel = "radial")
svm.nonlinear
heatmap(svm.nonlinear)
plot(svm.nonlinear)
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.1, 1, 0.02), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(0.1, 1, 0.1), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(1, 5, 1), gamma = seq(0.001, 0.06, 0.002),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(1, 5, 1), gamma = seq(0.01, 0.06, 0.005),
kernel = "radial")
linear_predict <- predict(svm.linear$best.model, test[,-1])
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(1, 5, 1), gamma = seq(0.004, 0.04, 0.004),
kernel = "radial")
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(1, 5, 1), gamma = seq(0.004, 0.04, 0.004),
kernel = "radial")
library(e1071)
train_5 <- read.csv("train.5.txt", header = FALSE)
train_6 <- read.csv("train.6.txt", header = FALSE)
labels <- rep(c(-1,1), c(nrow(train_5), nrow(train_6)))
labels <- as.factor(labels)
data_complete <- rbind(train_5, train_6)
data_complete <- cbind(labels, data_complete)
rows <- nrow(data_complete)
set.seed(2018)
test_samples <- sample(1:rows, 0.2*rows)
train <- data_complete[-test_samples, ]
test <- data_complete[test_samples, ]
svm.linear <- tune.svm(labels ~ ., data = train, cost = seq(0.001, 0.06, 0.002), kernel = "linear")
svm.linear
plot(svm.linear)
linear_predict <- predict(svm.linear$best.model, test[,-1])
linear_error <- mean(linear_predict != test[,1])
linear_error
svm.nonlinear <- tune.svm(labels ~ ., data = train, cost = seq(1, 10, 2), gamma = seq(0.004, 0.04, 0.004),
kernel = "radial")
?seq
pnorm(1.64)
1-pnorm(1.64)
dnorm(1.64)
packages.used=c("caret","gbm", "e1071", "DMwR", "nnet", "randomForest","OpenImageR","DT", "caTools", "EBImage", "mxnet", "pbapply", "ggthemes")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(caret)
library(gbm)
library(e1071)
library(DMwR)
library(randomForest)
library(nnet)
library(OpenImageR)
library(DT)
library(caTools)
#library(EBImage)
#library(mxnet)
library(pbapply)
library(ggthemes)
source("../lib/train.R")
source("../lib/test.R")
source("../lib/data_split.R")
LR_hog <- train_lr(train_hog)
LR_hog <- train_lr.cv(train_hog)
datasplit_hog <- data_split("hog_extraction1")
train_hog <- datasplit_hog$df_train
test_hog <- datasplit_hog$df_test
LR_hog <- train_lr.cv(train_hog)
save(LR_hog,file="../output/LR_hog.RData")
load("../output/LR_hog.RData")
LR_hog.time <- LR_hog$time
LR_hog.time
LR.test.result_hog <- test(LR_hog, test_hog)
LR.test.accuracy_hog <- mean(LR.test.result_hog == test_hog[,1])
LR.test.accuracy_hog
install.packages("mxnet")
library(mxnet)
load("../output/rf_SIFT.result.RData")
rf_SIFT.result.time <- rf_SIFT.result$time
rf_SIFT.result.time
rf_SIFT.test.result <- test(rf_SIFT.result, datasplit_sift$df_test)
read.csv('https://s3.amazonaws.com/orim-misc-data/assessment/subscribers.csv')
read.csv('https://s3.amazonaws.com/orim-misc-data/assessment/subscribers.csv')
fread('https://s3.amazonaws.com/orim-misc-data/assessment/subscribers.csv')
install.packages("data.table")
library(fread)
library(data.table)
fread('https://s3.amazonaws.com/orim-misc-data/assessment/subscribers.csv')
data1 <- read.csv('https://s3.amazonaws.com/orim-misc-data/assessment/subscribers.csv')
data1 <- fread('https://s3.amazonaws.com/orim-misc-data/assessment/subscribers.csv')
View(data1)
class(data1$Signup)
as.numeric(data1$Signup)
head(data1$Signup)
substring1 <- substr(data1$Signup,1,4)
head(substring1)
as.numeric(substring1)
new.data <- cbind(substring2, data1)
substring2 <- as.numeric(substring1)
new.data <- cbind(substring2, data1)
View(new.data)
substring2 <- substr(data1$Signup,6,7)
head(substring2)
substring3 <- substr(data1$Signup,6,7)
substring2 <- as.numeric(substring1)
substring4 <- as.numeric(substring3)
substring3 <- substr(data1$Signup,6,7)
head(substring2)
substring4 <- as.numeric(substring3)
new.data1 <- cbind(substring4, new.data)
unique(new.data1$substring2)
years <- unique(new.data1$substring2)
year_2014 <- new.data1[new.data1$substring2 == 2014, ]
year_2015 <- new.data1[new.data1$substring2 == 2015, ]
year_2016 <- new.data1[new.data1$substring2 == 2016, ]
year_2017 <- new.data1[new.data1$substring2 == 2017, ]
year_2014_1 <- year_2014[year_2014$]
substring4
load("C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project3-Group6/output/XGBoost_RGB.result.RData")
model3
library(xgboost)
install.packages("xgboost")
library(xgboost)
df <- read.csv('rgb_feature.csv', header=FALSE)
labels <- read.csv('label_train.csv')
setwd("C:/StudyLife/Columbia/STAT 5243/Project 2/Spring2018-Project3-Group6/output")
library(xgboost)
df <- read.csv('rgb_feature.csv', header=FALSE)
labels <- read.csv('label_train.csv')
df$label <- as.factor(labels$label)
levels(df$label)[levels(df$label)=="1"] <- "0"
levels(df$label)[levels(df$label)=="2"] <- "1"
levels(df$label)[levels(df$label)=="3"] <- "2"
df.features <- df[, -which(colnames(df) == 'label')]
df.features$V1 <- NULL
df.target <- df$label
xgb.tune <- expand.grid(eta=c(0.3, 0.5, 0.7),
gamma=c(0, 0.5, 1),
max_depth = c(2, 3, 4, 5, 10))
xgb.tune$accuracy <- numeric(nrow(xgb.tune))
set.seed(032718)
for(i in 1:nrow(xgb.tune)){
t1 = Sys.time()
print(paste('Starting iteration', i, 'of', nrow(xgb.tune), ':'))
param_list <- list(max_depth=xgb.tune$max_depth[i],
eta=xgb.tune$eta[i],
gamma = xgb.tune$gamma[i],
silent=1,
nthread=2,
objective='multi:softmax')
model <- xgb.cv(data = as.matrix(df.features),
nrounds = 50,
nfold = 5,
metrics = list("merror"),
label = df.target,
params = param_list,
num_class = 4)
# Takes mean of 10 training rounds with highest classification rate
xgb.tune$accuracy[i] <- 1-(mean(sort(model$evaluation_log$test_merror_mean)[1:10]))
t2 = Sys.time()
print(paste('Iteration', i, 'took :', (t2-t1), 'seconds'))
}
best.params <- xgb.tune[which.max(xgb.tune$accuracy),]
best.params
library(xgboost)
df <- read.csv('SIFT_train.csv', header=FALSE)
labels <- read.csv('label_train.csv')
df$label <- as.factor(labels$label)
levels(df$label)[levels(df$label)=="1"] <- "0"
levels(df$label)[levels(df$label)=="2"] <- "1"
levels(df$label)[levels(df$label)=="3"] <- "2"
df.features <- df[, -which(colnames(df) == 'label')]
df.features$V1 <- NULL
df.target <- df$label
df.features <- df[, -which(colnames(df) == 'label')]
df.features$V1 <- NULL
df.target <- df$label
xgb.tune <- expand.grid(eta=c(0.3, 0.5, 0.7),
gamma=c(0, 0.5, 1),
max_depth = c(2, 3, 4, 5, 10))
xgb.tune$accuracy <- numeric(nrow(xgb.tune))
set.seed(032718)
for(i in 1:nrow(xgb.tune)){
t1 = Sys.time()
print(paste('Starting iteration', i, 'of', nrow(xgb.tune), ':'))
param_list <- list(max_depth=xgb.tune$max_depth[i],
eta=xgb.tune$eta[i],
gamma = xgb.tune$gamma[i],
silent=1,
nthread=2,
objective='multi:softmax')
model <- xgb.cv(data = as.matrix(df.features),
nrounds = 50,
nfold = 5,
metrics = list("merror"),
label = df.target,
params = param_list,
num_class = 4)
# Takes mean of 10 training rounds with highest classification rate
xgb.tune$accuracy[i] <- 1-(mean(sort(model$evaluation_log$test_merror_mean)[1:10]))
t2 = Sys.time()
print(paste('Iteration', i, 'took :', (t2-t1), 'seconds'))
}
best.params <- xgb.tune[which.max(xgb.tune$accuracy),]
best.params
