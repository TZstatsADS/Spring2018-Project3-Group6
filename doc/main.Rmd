---
title: "main"
output:
  word_document: default
  html_notebook: default
---

# Step 0. Install, Load packages 

Because of using the package caret, when we trained model we also perform cross validation by using the `tuneGrid` and `traincontrol` arguement, so we ignore creating cross validation R file.

```{r}
packages.used=c("caret","gbm","EBImage","e1071", "DMwR", "nnet", "randomForest","OpenImageR","DT", "caTools","mxnet", "pbapply", "ggthemes")

packages.needed=setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))

if(length(packages.needed)>0){
 install.packages(packages.needed, dependencies = TRUE)
}

library(caret)
library(gbm)
library(EBImage)
library(caret)
library(gbm)
library(e1071)
library(DMwR)
library(randomForest)
library(nnet)
library(OpenImageR)
library(DT)
library(caTools)
library(EBImage)
#library(mxnet)
library(pbapply)
library(ggthemes)

source("../lib/train.R")
source("../lib/test.R")
source("../lib/data_split.R")
```

# Step 1. Model Comparsion Based on SIFT Feature

# Step 1.1. Load Feature

 We devided the whole training set into 'df_train'-training data (80%)  & 'df_test'-testing data (20%)
 
```{r, eval=FALSE}
datasplit_hog <- data_split("SIFT") 
train_sift<- datasplit_hog$df_train
# test_sift <- datasplit_hog$df_test
```

# Step 1.2. Baseline-GBM (GBM+SIFT)

```{r}
load("../output/baseline.result.RData")
baseline.time <- baseline.result$time
baseline.time

# training.accuracy <- 1-baseline.result[["fit"]][["train.error"]][400]
# training.accuracy
baseline.test.result <- test_gbm(baseline.result, datasplit_hog$df_test)
baseline.test.accuracy <- 1 - mean(baseline.test.result != datasplit_hog$df_test[,1])
baseline.test.accuracy
```

# Step 1.3. SVM (SVM + SIFT)

# Step 1.3.1 Training Process of SVM model
 
```{r, eval = FALSE}
svm_SIFT.result <- train_svm(SIFT_train)
save(svm_SIFT.result,file="../output/svm_SIFT.result.RData")
```

# Step 1.3.2 Test of SVM model 

```{r}
load("../output/svm_SIFT.result.RData")
svm_SIFT.result.time <- svm_SIFT.result$time
svm_SIFT.result.time

svm_SIFT.test.result <- test(svm_SIFT.result, datasplit_hog$df_test)
svm_SIFT.test.accuracy <- 1 - mean(svm_SIFT.test.result != datasplit_hog$df_test[,1])
svm_SIFT.test.accuracy
```

# Step 1.4. Random Forest (Random Forest + SIFT)

# Step 1.4.1 Training Process of SVM model
 
```{r, eval = FALSE}
rf_SIFT.result <- train_rf(SIFT_train)
save(rf_SIFT.result,file="../output/rf_SIFT.result.RData")
```

# Step 1.4.2 Test of Random Forest Model

```{r}
load("../output/rf_SIFT.result.RData")
rf_SIFT.result.time <- rf_SIFT.result$time
rf_SIFT.result.time


rf_SIFT.test.result <- test(rf_SIFT.result, datasplit_hog$df_test)
rf_SIFT.test.accuracy <- 1 - mean(rf_SIFT.test.result != datasplit_hog$df_test[,1])
rf_SIFT.test.accuracy
```

# Step 1.5. Logistic Regression (Logistic Regression + SIFT)

# Step 1.5.1 Training Process of Logistic Regression model
 
```{r, eval = FALSE}
lr_SIFT.result <- train_lr(SIFT_train)
save(lr_SIFT.result,file="../output/lr_SIFT.result.RData")
```

# Step 1.5.2 Test of Logistic Regression Model

```{r}
load("../output/lr_SIFT.result.RData")
lr_SIFT.result.time <- lr_SIFT.result$time
lr_SIFT.result.time

lr_SIFT.test.result <- test(lr_SIFT.result, datasplit_hog$df_test)
lr_SIFT.test.accuracy <- 1 - mean(lr_SIFT.test.result != datasplit_hog$df_test[,1])
lr_SIFT.test.accuracy
```

# Step 1.6. Neural network (Neural network+SIFT)

```{r}
library(neuralnet)
library(nnet)
df <- read.csv('SIFT_train.csv', header=FALSE)
labels <- read.csv('label_train.csv')
df$label <- as.factor(labels$label) #add a column called label to df
df$V1 <- NULL

# Scale Data
scl <- function(x){(x-min(x))/(max(x)-min(x))}
target.i <- which(colnames(df)=='label')
df[,-target.i] <- data.frame(lapply(df[,-target.i], scl))

# Dummy code Outcome Variable
# Create 3 variables that pertains the labels are that being predicted
df$l1 <- ifelse(df$label=="1", 1,0)
df$l2 <- ifelse(df$label=="2", 1,0)
df$l3 <- ifelse(df$label=="3", 1,0)
df$label <- NULL

# Split into training and test 
set.seed(031818)
test.i <- sample(1:nrow(df), .3*nrow(df), replace=FALSE)
test.data <- df[test.i,]
train.data <- df[-test.i,]
test.target <- labels[test.i, 'label']

# Set up formula
n <- names(train.data)
f <- as.formula(paste("l1 + l2 + l3 ~",paste(n[!n %in% c("l1","l2","l3")], collapse="+")))

# Gridsearch: Creating hyperparameter, allows to train on all combinations of tuning parameters
nn.tune <- expand.grid(hidden=seq(0,10, by=1),
                       act.fct=c("logistic"))
nn.tune$accuracy <- numeric(nrow(nn.tune))

for(i in 1:nrow(nn.tune)){
  t1 = Sys.time()
nn <- neuralnet(f,
                data=train.data,
                hidden=nn.tune$hidden[i],
                act.fct=nn.tune$act.fct[i],
                linear.output=FALSE,
                lifesign="minimal")
pr.nn <- compute(nn, test.data[1:2000])
nn.pred <- apply(pr.nn$net.result, 1, which.max)
nn.tune$accuracy[i] <- mean(nn.pred==test.target)
t2 <- Sys.time()
print(t2-t1)
}

# Best parameter
best.params <- nn.tune[which.max(nn.tune$accuracy), c('hidden', 'act.fct')]
#Retrain our best model
t1_1 = Sys.time()
nn <- neuralnet(f,
                data=train.data,
                hidden=best.params$hidden,
                act.fct=best.params$act.fct,
                linear.output=FALSE,
                lifesign="minimal")
pr.nn <- compute(nn, test.data[1:2000])
nn.pred <- apply(pr.nn$net.result, 1, which.max)
best.accuracy <- mean(nn.pred==test.target)
t2_1 <- Sys.time()
print(t2_1-t1_1)
print(paste('Our best accuracy was', best.accuracy))
```

# Step 1.7. XGBoost ( XGBoost + SIFT)

```{r}
library(xgboost)
df <- read.csv('SIFT_train.csv', header=FALSE)
labels <- read.csv('label_train.csv')
df$label <- as.factor(labels$label)
df$V1 <- NULL

# Relabel factors for XGBoost specific num_classes requirement
levels(df$label)[levels(df$label)=="1"] <- "0"
levels(df$label)[levels(df$label)=="2"] <- "1"
levels(df$label)[levels(df$label)=="3"] <- "2"

#XGBoost Algorithm
set.seed(031918)
test.i <- sample(1:nrow(df), .3*nrow(df), replace=FALSE)
test.data <- df[test.i,]
train.data <- df[-test.i,]

target.i <- which(colnames(df) == 'label')
train.data <- df[-test.i, -target.i]
train.target <- df[-test.i, target.i]

t1=Sys.time()
model <- xgboost(data = as.matrix(train.data), label = train.target,
                 nrounds = 2, objective = "multi:softmax", num_class = 4)

xgb.preds <- predict(model, as.matrix(test.data[,-target.i]))
mean(xgb.preds == as.numeric(test.data$label))
t2=Sys.time()
t2-t1
```
